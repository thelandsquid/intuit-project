{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-VGG16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy_EM2K2t30S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ad97c12e-dc31-4931-96af-458b56d446c2"
      },
      "source": [
        "!git clone --single-branch --branch production https://github.com/bippity/intuit-project.git\n",
        "%cd intuit-project/Current\\ CNN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'intuit-project'...\n",
            "remote: Enumerating objects: 3936, done.\u001b[K\n",
            "remote: Total 3936 (delta 0), reused 0 (delta 0), pack-reused 3936\u001b[K\n",
            "Receiving objects: 100% (3936/3936), 1.68 GiB | 37.62 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "Checking out files: 100% (3953/3953), done.\n",
            "/content/intuit-project/Current CNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhAFsnPJgqer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import os \n",
        "import cv2\n",
        "import numpy as np\n",
        "import random \n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6gQSkzug1iT",
        "colab_type": "text"
      },
      "source": [
        "Create training data, resizing all images to 200x200 and using color(R,G,B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXU_tcMRg5NN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATADIR = \"Kaggle\"\n",
        "CATEGORIES = [\"Good\", \"Bad\"]\n",
        "training_data = []\n",
        "IMG_SIZE = 200         # This is the size we are using \n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:                 # loop threw each folder with in W2 folder  \n",
        "        path = os.path.join(DATADIR,category)   # Path to folder \n",
        "        class_num = CATEGORIES.index(category)  # labeling the data based on folder \n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR) # converts the image to an array \n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))   # Resize to normalize data size \n",
        "                training_data.append([new_array, class_num]) # adds it to are traning data with the label \n",
        "            except Exception as e:\n",
        "                pass\n",
        "            \n",
        "create_training_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJVdoAnthXYA",
        "colab_type": "text"
      },
      "source": [
        "Shuffling data before feeding it to the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9tAoTXshZ2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf1JIHvQhcnN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Separating the features from the labels and converting them to a np array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOhVA9hzhfF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features, label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "    \n",
        "X = np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "y = np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qqpD0rAhisa",
        "colab_type": "text"
      },
      "source": [
        "We start by normalizing the data by scaling it, min is 0 and max is 255 for pixel data So we will divide it by 255, Keras also has a built in function to do this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIVrXAFlhkPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X/255 #255 pixels max for pixel data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqmFDOxhmCJ",
        "colab_type": "text"
      },
      "source": [
        "Instantiate a VGG16 model that is preloaded with weights,\n",
        "\n",
        "We tell it the image size and that the images will be in color(3)\n",
        "\n",
        "include_top = False will not include the classification layer, we will add one ourselves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH4QCbtchnZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eaff89bd-9648-435c-82a1-e17f8de1416b"
      },
      "source": [
        "IMG_SIZE = 200\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "VGG16_MODEL=tf.keras.applications.VGG16(input_shape = IMG_SHAPE,\n",
        "                                               include_top = False,\n",
        "                                               weights = 'imagenet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNAtPj_QhqQg",
        "colab_type": "text"
      },
      "source": [
        "Summary of the layers VGG16 includes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcvHk_8ahroj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "2517b971-c564-4e1b-df46-2f026ae955de"
      },
      "source": [
        "VGG16_MODEL.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200, 200, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 200, 200, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 200, 200, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 100, 100, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 100, 100, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 100, 100, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 50, 50, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 50, 50, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 25, 25, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 25, 25, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHKu2zvLhuSK",
        "colab_type": "text"
      },
      "source": [
        "We are freezing the VGG16 model so that way the weights in the given model will not update. Also including 2 more layers, one being our output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-f9axbGhv6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VGG16_MODEL.trainable = False\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "prediction_layer = tf.keras.layers.Dense(2,activation='softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHLkg3iJhxe-",
        "colab_type": "text"
      },
      "source": [
        "Will convert to a sequential model and combine the last two layer we made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocBqfGsih4XL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  VGG16_MODEL,\n",
        "  global_average_layer,\n",
        "  prediction_layer\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv78SlHxh03p",
        "colab_type": "text"
      },
      "source": [
        "Compile our model using an 'adam' optimizer and 'sparese categorical crossentropy' for the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td_e_X0Dh804",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2uo_Dn8iBI8",
        "colab_type": "text"
      },
      "source": [
        "Time to fit the model with 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWrnxiHQiDpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "772bd8ea-65ed-45ef-84b8-870de3474816"
      },
      "source": [
        "model.fit(X, y, batch_size = 12, epochs = 5, validation_split = .1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "281/281 [==============================] - 12s 44ms/step - loss: 0.2635 - accuracy: 0.9215 - val_loss: 0.1977 - val_accuracy: 0.9358\n",
            "Epoch 2/5\n",
            "281/281 [==============================] - 11s 40ms/step - loss: 0.1712 - accuracy: 0.9417 - val_loss: 0.1671 - val_accuracy: 0.9358\n",
            "Epoch 3/5\n",
            "281/281 [==============================] - 11s 41ms/step - loss: 0.1478 - accuracy: 0.9482 - val_loss: 0.1564 - val_accuracy: 0.9358\n",
            "Epoch 4/5\n",
            "281/281 [==============================] - 11s 40ms/step - loss: 0.1349 - accuracy: 0.9530 - val_loss: 0.1439 - val_accuracy: 0.9385\n",
            "Epoch 5/5\n",
            "281/281 [==============================] - 11s 41ms/step - loss: 0.1261 - accuracy: 0.9545 - val_loss: 0.1331 - val_accuracy: 0.9519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb9c00aa0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iby5BPqLiH-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "52b2e090-9fec-4d63-a8da-cc88bf09bb48"
      },
      "source": [
        "model.save(\"VGG16_v1\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: VGG16_v1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF6-O5q5p4_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5024f31d-ab67-4c6a-a11d-6a4d5d8e03ac"
      },
      "source": [
        "#Zip model to download\n",
        "!zip -r VGG16_v1.zip VGG16_v1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: VGG16_v1/ (stored 0%)\n",
            "  adding: VGG16_v1/variables/ (stored 0%)\n",
            "  adding: VGG16_v1/variables/variables.data-00001-of-00002 (deflated 7%)\n",
            "  adding: VGG16_v1/variables/variables.index (deflated 64%)\n",
            "  adding: VGG16_v1/variables/variables.data-00000-of-00002 (deflated 80%)\n",
            "  adding: VGG16_v1/assets/ (stored 0%)\n",
            "  adding: VGG16_v1/saved_model.pb (deflated 92%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}